{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8012 images belonging to 2 classes.\n",
      "Found 2003 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, Concatenate, Multiply, Conv2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Directory paths\n",
    "data_dir = r'C:/Users/rishi/OneDrive/Desktop/cnnmodel'\n",
    "\n",
    "# Image dimensions\n",
    "img_width, img_height = 224, 224\n",
    "batch_size = 32\n",
    "\n",
    "# Data Augmentation and Rescaling\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # 80% training, 20% validation\n",
    ")\n",
    "\n",
    "# Data Generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_attention(input_feature):\n",
    "    avg_pool = tf.reduce_mean(input_feature, axis=-1, keepdims=True)\n",
    "    max_pool = tf.reduce_max(input_feature, axis=-1, keepdims=True)\n",
    "    concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
    "    attention = Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n",
    "    return Multiply()([input_feature, attention])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m resnet_output \u001b[38;5;241m=\u001b[39m resnet\u001b[38;5;241m.\u001b[39moutput  \u001b[38;5;66;03m# Shape: (None, 7, 7, 2048) for ResNet50\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Apply attention mechanism to both outputs\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m efficient_attention \u001b[38;5;241m=\u001b[39m \u001b[43mspatial_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mefficient_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m resnet_attention \u001b[38;5;241m=\u001b[39m spatial_attention(resnet_output)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Merge the outputs\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 7\u001b[0m, in \u001b[0;36mspatial_attention\u001b[1;34m(input_feature)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspatial_attention\u001b[39m(input_feature):\n\u001b[1;32m----> 7\u001b[0m     input_feature \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_feature\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Ensure input is a Tensor\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     avg_pool \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_mean(input_feature, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m     max_pool \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_max(input_feature, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\rishi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\rishi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\keras_tensor.py:91\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[1;34m(self, dtype, name)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 91\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    103\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout, Concatenate, Multiply, Conv2D\n",
    "from tensorflow.keras.applications import EfficientNetB0, ResNet50\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    input_feature = tf.convert_to_tensor(input_feature)  # Ensure input is a Tensor\n",
    "    avg_pool = tf.reduce_mean(input_feature, axis=-1, keepdims=True)\n",
    "    max_pool = tf.reduce_max(input_feature, axis=-1, keepdims=True)\n",
    "    concat = tf.concat([avg_pool, max_pool], axis=-1)\n",
    "    attention = Conv2D(1, kernel_size=7, padding='same', activation='sigmoid')(concat)\n",
    "    return Multiply()([input_feature, attention])\n",
    "\n",
    "\n",
    "# Input layer\n",
    "input_shape = (224, 224, 3)  # Image dimensions\n",
    "input_tensor = Input(shape=input_shape)\n",
    "\n",
    "# EfficientNetB0 and ResNet50 backbones\n",
    "efficient_net = EfficientNetB0(include_top=False, weights='imagenet', input_tensor=input_tensor, pooling=None)\n",
    "resnet = ResNet50(include_top=False, weights='imagenet', input_tensor=input_tensor, pooling=None)\n",
    "\n",
    "# Ensure same feature map sizes\n",
    "efficient_output = efficient_net.output  # Shape: (None, 7, 7, 1280) for EfficientNetB0\n",
    "resnet_output = resnet.output  # Shape: (None, 7, 7, 2048) for ResNet50\n",
    "\n",
    "# Apply attention mechanism to both outputs\n",
    "efficient_attention = spatial_attention(efficient_output)\n",
    "resnet_attention = spatial_attention(resnet_output)\n",
    "\n",
    "# Merge the outputs\n",
    "merged = Concatenate()([efficient_attention, resnet_attention])\n",
    "\n",
    "# Global Average Pooling and Dropout\n",
    "x = GlobalAveragePooling2D()(merged)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# Output layer (binary classification)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Build and compile the model\n",
    "model = Model(inputs=input_tensor, outputs=output)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for saving the best model and early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "checkpoint = ModelCheckpoint('best_skin_cancer_model.h5', monitor='val_accuracy', save_best_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, checkpoint],\n",
    "    steps_per_epoch=train_generator.samples // batch_size,\n",
    "    validation_steps=validation_generator.samples // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set\n",
    "val_loss, val_acc = model.evaluate(validation_generator)\n",
    "print(f'Validation Loss: {val_loss}')\n",
    "print(f'Validation Accuracy: {val_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "y_pred = model.predict(validation_generator)\n",
    "y_pred = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print(classification_report(validation_generator.classes, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# Load and preprocess a new image for prediction\n",
    "def predict_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(img_width, img_height))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array /= 255.  # Normalize to [0, 1] range\n",
    "\n",
    "    # Predict\n",
    "    prediction = model.predict(img_array)\n",
    "    \n",
    "    # Interpret prediction\n",
    "    if prediction > 0.5:\n",
    "        print(\"Prediction: Positive for skin cancer\")\n",
    "    else:\n",
    "        print(\"Prediction: Negative for skin cancer\")\n",
    "\n",
    "# Example: Upload an image and predict\n",
    "image_path = r'C:/Users/rishi/OneDrive/Desktop/cnnmodel/HAM10000_images_part_1/ISIC_0024306.jpg'\n",
    "predict_image(image_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
